{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_intro",
   "metadata": {},
   "source": [
    "# Factorization Machine (FM) Recommender System\n",
    "This notebook implements a 2-way Factorization Machine from scratch using NumPy.\n",
    "It evaluates the model using both **RMSE** (rating accuracy) and **NDCG** (ranking quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load_libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rami/git/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000209 ratings and 3883 movies.\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"odedgolden/movielens-1m-dataset\")\n",
    "\n",
    "ratings_cols = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "movies_cols = ['MovieID', 'Title']\n",
    "\n",
    "ratings_df = pd.read_csv(os.path.join(path, 'ratings.dat'), sep='::', names=ratings_cols, encoding='latin-1', engine='python')\n",
    "movies_df = pd.read_csv(os.path.join(path, 'movies.dat'), sep='::', names=movies_cols, usecols=[0, 1], encoding='latin-1', engine='python')\n",
    "\n",
    "print(f\"Loaded {len(ratings_df)} ratings and {len(movies_df)} movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prep_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users: 6040, Total Movies: 3706, Total Features: 9746\n",
      "Train samples: 800167, Test samples: 200042\n",
      "\n",
      "Building Training Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Matrix: 100%|██████████| 800167/800167 [00:02<00:00, 362719.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Testing Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Matrix: 100%|██████████| 200042/200042 [00:00<00:00, 370419.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global Mappings (Shared between Train & Test)\n",
    "# This ensures consistent Feature IDs across both sets.\n",
    "user_map = {id_: idx for idx, id_ in enumerate(sorted(ratings_df['UserID'].unique()))}\n",
    "movie_map = {id_: idx for idx, id_ in enumerate(sorted(ratings_df['MovieID'].unique()))}\n",
    "\n",
    "n_users = len(user_map)\n",
    "n_movies = len(movie_map)\n",
    "n_features = n_users + n_movies\n",
    "\n",
    "print(f\"Total Users: {n_users}, Total Movies: {n_movies}, Total Features: {n_features}\")\n",
    "\n",
    "# Split DATAFRAME first (Required for NDCG user grouping)\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "# Conversion Function (DataFrame -> Sparse Matrix)\n",
    "def df_to_sparse_matrix(df, user_map, movie_map, n_users, n_features):\n",
    "    \"\"\"Converts a ratings DataFrame to a sparse feature matrix X and target y.\"\"\"\n",
    "    n_samples = df.shape[0]\n",
    "    X = lil_matrix((n_samples, n_features), dtype=np.int8)\n",
    "    y = df['Rating'].values.astype(np.float32)\n",
    "\n",
    "    for i, row in enumerate(tqdm(df.itertuples(index=False), total=n_samples, desc=\"Building Matrix\")):\n",
    "        # User one-hot feature\n",
    "        X[i, user_map[row.UserID]] = 1\n",
    "        # Movie one-hot feature (offset by number of users)\n",
    "        X[i, n_users + movie_map[row.MovieID]] = 1\n",
    "\n",
    "    return X.tocsr(), y # Convert to CSR for fast arithmetic later\n",
    "\n",
    "\n",
    "print(\"\\nBuilding Training Matrix...\")\n",
    "X_train, y_train = df_to_sparse_matrix(train_df, user_map, movie_map, n_users, n_features)\n",
    "print(\"Building Testing Matrix...\")\n",
    "X_test, y_test = df_to_sparse_matrix(test_df, user_map, movie_map, n_users, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fm_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FactorizationMachine:\n",
    "    \"\"\"\n",
    "    A 2-Way Factorization Machine implemented with NumPy.\n",
    "    Optimized for sparse inputs and vectorized batch predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, k=10, learning_rate=0.01, l2_reg=0.1, n_epochs=5):\n",
    "        self.n_features = n_features\n",
    "        self.k = k\n",
    "        self.lr = learning_rate\n",
    "        self.l2 = l2_reg\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Parameters\n",
    "        self.w0 = 0.0\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.V = np.random.normal(0, 0.1, (n_features, k))\n",
    "\n",
    "        # For early stopping / best model saving\n",
    "        self.best_test_rmse = float('inf')\n",
    "        self.best_w0 = self.w0\n",
    "        self.best_w = self.w.copy()\n",
    "        self.best_V = self.V.copy()\n",
    "\n",
    "    def predict_batch(self, X):\n",
    "        \"\"\"Optimized batch prediction for CSR matrix X.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        # Linear part (Global bias + User bias + Movie bias)\n",
    "        pred = np.full(n_samples, self.w0) + X.dot(self.w)\n",
    "        \n",
    "        # Interaction part (User vector dot Movie vector)\n",
    "        if self.k > 0:\n",
    "            # Efficient calculation using FM identity: 0.5 * [ (sum(Vi*xi))^2 - sum((Vi*xi)^2) ]\n",
    "            term1 = (X.dot(self.V)) ** 2\n",
    "            term2 = X.dot(self.V ** 2)\n",
    "            interaction = 0.5 * np.sum(term1 - term2, axis=1)\n",
    "            pred += interaction\n",
    "        return pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Trains the model using Stochastic Gradient Descent (SGD).\"\"\"\n",
    "        print(\"\\nStarting training...\")\n",
    "        train_indices = np.arange(X_train.shape[0])\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(train_indices)\n",
    "            \n",
    "            for idx in tqdm(train_indices, desc=f\"Epoch {epoch+1}/{self.n_epochs}\", leave=False):\n",
    "                x = X_train[idx]\n",
    "                y_true = y_train[idx]\n",
    "\n",
    "                # --- SGD Step (Optimized for single sample) ---\n",
    "                active_idx = x.nonzero()[1]\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = self.w0 + np.sum(self.w[active_idx])\n",
    "                if self.k > 0:\n",
    "                    v_active = self.V[active_idx]\n",
    "                    sum_v = np.sum(v_active, axis=0)\n",
    "                    interaction = 0.5 * np.sum(sum_v**2 - np.sum(v_active**2, axis=0))\n",
    "                    pred += interaction\n",
    "\n",
    "                error = pred - y_true\n",
    "\n",
    "                # Backward pass (Updates)\n",
    "                self.w0 -= self.lr * error\n",
    "                \n",
    "                w_grad = error + self.l2 * self.w[active_idx]\n",
    "                self.w[active_idx] -= self.lr * w_grad\n",
    "\n",
    "                if self.k > 0:\n",
    "                    v_sum = np.sum(self.V[active_idx], axis=0)\n",
    "                    # Gradient for V uses the pre-calculated sum for efficiency\n",
    "                    v_grad = error * (v_sum - self.V[active_idx]) + self.l2 * self.V[active_idx]\n",
    "                    self.V[active_idx] -= self.lr * v_grad\n",
    "\n",
    "            # Evaluation at end of epoch\n",
    "            train_rmse = self.evaluate_rmse(X_train, y_train)\n",
    "            test_rmse = self.evaluate_rmse(X_test, y_test)\n",
    "            print(f\"Epoch {epoch+1}: Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if test_rmse < self.best_test_rmse:\n",
    "                self.best_test_rmse = test_rmse\n",
    "                self.best_w0 = self.w0\n",
    "                self.best_w = self.w.copy()\n",
    "                self.best_V = self.V.copy()\n",
    "                print(f\" -> New best model saved (Test RMSE: {test_rmse:.4f})\")\n",
    "\n",
    "        # Restore best model\n",
    "        self.w0 = self.best_w0\n",
    "        self.w = self.best_w\n",
    "        self.V = self.best_V\n",
    "        print(\"Training complete. Best model restored.\")\n",
    "\n",
    "    def evaluate_rmse(self, X, y_true):\n",
    "        y_pred = self.predict_batch(X)\n",
    "        # Clip predictions to valid rating range [1, 5] for realistic RMSE\n",
    "        y_pred = np.clip(y_pred, 1.0, 5.0)\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ndcg_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_ndcg(model, test_df, user_map, movie_map, n_users, k=10):\n",
    "    \"\"\"\n",
    "    Calculates average NDCG@k for users in the test set.\n",
    "    This measures RANKING quality (how well top recommendations match true preferences).\n",
    "    \"\"\"\n",
    "    print(f\"\\nCalculating NDCG@{k} on Test Set...\")\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # Group test data by user to evaluate their personal ranking\n",
    "    grouped = test_df.groupby('UserID')\n",
    "    \n",
    "    for user_id, group in tqdm(grouped, desc=\"Evaluated Users\"):\n",
    "        # Need at least 2 items to have a meaningful ranking\n",
    "        if len(group) < 2: continue\n",
    "\n",
    "        y_true = group['Rating'].values\n",
    "        y_true_2d = [y_true]\n",
    "\n",
    "        # Prepare features for this user's test items\n",
    "        u_idx = user_map[user_id]\n",
    "        m_indices = [movie_map[mid] for mid in group['MovieID'].values]\n",
    "        n_samples = len(group)\n",
    "\n",
    "        # sparse matrix for this small batch\n",
    "        rows = np.concatenate([np.arange(n_samples), np.arange(n_samples)])\n",
    "        cols = np.concatenate([np.full(n_samples, u_idx), np.array(m_indices) + n_users])\n",
    "        data = np.ones(len(rows))\n",
    "        X_user = csr_matrix((data, (rows, cols)), shape=(n_samples, model.n_features))\n",
    "\n",
    "        # Predict ranking scores\n",
    "        y_pred = model.predict_batch(X_user)\n",
    "        y_pred_2d = [y_pred]\n",
    "\n",
    "        # Calculate NDCG@k for this user\n",
    "        score = ndcg_score(y_true_2d, y_pred_2d, k=k)\n",
    "        ndcg_scores.append(score)\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"Average NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train_exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSE: 0.9340 | Test RMSE: 0.9441\n",
      " -> New best model saved (Test RMSE: 0.9441)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train RMSE: 0.9174 | Test RMSE: 0.9294\n",
      " -> New best model saved (Test RMSE: 0.9294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train RMSE: 0.9087 | Test RMSE: 0.9220\n",
      " -> New best model saved (Test RMSE: 0.9220)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train RMSE: 0.9064 | Test RMSE: 0.9201\n",
      " -> New best model saved (Test RMSE: 0.9201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train RMSE: 0.9031 | Test RMSE: 0.9173\n",
      " -> New best model saved (Test RMSE: 0.9173)\n",
      "Training complete. Best model restored.\n",
      "\n",
      "Final Test RMSE: 0.9173\n",
      "\n",
      "Calculating NDCG@10 on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluated Users: 100%|██████████| 6038/6038 [00:03<00:00, 1728.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@10: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fm = FactorizationMachine(\n",
    "    n_features=n_features,\n",
    "    k=15,                 # Latent dimension size\n",
    "    learning_rate=0.005, \n",
    "    l2_reg=0.1, \n",
    "    n_epochs=5\n",
    ")\n",
    "\n",
    "fm.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"\\nFinal Test RMSE: {fm.best_test_rmse:.4f}\")\n",
    "\n",
    "final_ndcg = evaluate_ndcg(fm, test_df, user_map, movie_map, n_users, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rec_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(user_id_original, n_recommends, fm_model, movies_df, ratings_df, user_map, movie_map, n_users):\n",
    "    \"\"\"Generates top N movie recommendations for an existing user using batch prediction.\"\"\"\n",
    "    if user_id_original not in user_map:\n",
    "        print(f\"User ID {user_id_original} not found.\")\n",
    "        return None\n",
    "    u_idx = user_map[user_id_original]\n",
    "\n",
    "    # unseen movies\n",
    "    seen_ids = ratings_df[ratings_df['UserID'] == user_id_original]['MovieID'].unique()\n",
    "    all_ids = movies_df['MovieID'].unique()\n",
    "    unseen_ids = np.setdiff1d(all_ids, seen_ids)\n",
    "    unseen_ids = [mid for mid in unseen_ids if mid in movie_map]\n",
    "    n_unseen = len(unseen_ids)\n",
    "    \n",
    "    if n_unseen == 0: return None\n",
    "\n",
    "    # Unique row index for each prediction\n",
    "    row_indices = np.arange(n_unseen) \n",
    "    user_col_indices = np.full(n_unseen, u_idx)\n",
    "    movie_col_indices = np.array([movie_map[mid] for mid in unseen_ids]) + n_users\n",
    "\n",
    "    # sparse matrix\n",
    "    rows = np.concatenate([row_indices, row_indices])\n",
    "    cols = np.concatenate([user_col_indices, movie_col_indices])\n",
    "    data_vals = np.ones(len(rows), dtype=np.int8)\n",
    "    X_pred = csr_matrix((data_vals, (rows, cols)), shape=(n_unseen, fm_model.n_features))\n",
    "\n",
    "    # Predict & Sort\n",
    "    predicted_ratings = fm_model.predict_batch(X_pred)\n",
    "    movie_rating_pairs = sorted(zip(unseen_ids, predicted_ratings), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top N details\n",
    "    top_n_ids = [mid for mid, _ in movie_rating_pairs[:n_recommends]]\n",
    "    return movies_df.set_index('MovieID').loc[top_n_ids].reset_index()\n",
    "\n",
    "def get_cold_start_recommendations(n_recommends, ratings_df, movies_df):\n",
    "    \"\"\"Popularity-based recommendations for new users.\"\"\"\n",
    "    print(\"Generating cold-start recommendations...\")\n",
    "    top_popular_ids = ratings_df['MovieID'].value_counts().head(n_recommends).index\n",
    "    return movies_df.set_index('MovieID').loc[top_popular_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "test_rec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Recommendations for User 4 ---\n",
      "   MovieID                                              Title\n",
      "0     2019  Seven Samurai (The Magnificent Seven) (Shichin...\n",
      "1      318                   Shawshank Redemption, The (1994)\n",
      "2      745                              Close Shave, A (1995)\n",
      "3     1148                         Wrong Trousers, The (1993)\n",
      "4      922      Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)\n",
      "\n",
      "--- Cold Start Recommendations ---\n",
      "Generating cold-start recommendations...\n",
      "   MovieID                                              Title\n",
      "0     2858                             American Beauty (1999)\n",
      "1      260          Star Wars: Episode IV - A New Hope (1977)\n",
      "2     1196  Star Wars: Episode V - The Empire Strikes Back...\n",
      "3     1210  Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "4      480                               Jurassic Park (1993)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_id_existing = 4\n",
    "print(f\"--- Recommendations for User {user_id_existing} ---\")\n",
    "print(get_top_n_recommendations(user_id_existing, 5, fm, movies_df, ratings_df, user_map, movie_map, n_users))\n",
    "\n",
    "print(f\"\\n--- Cold Start Recommendations ---\")\n",
    "print(get_cold_start_recommendations(5, ratings_df, movies_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
